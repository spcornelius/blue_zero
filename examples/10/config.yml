qnet:
  id: 'simple'
  depth: 5
  kernel_size: 3
  num_feat: 32
  num_hidden: 32
  bias: true
  batchnorm: false
mode:
  id: 0
  # blank = no normalization of rewards
  reward_norm: 'side_length'
  shape_rewards: false
replay:
  capacity: 1_000_000
  step_diff: 1
training:
  batch_size: 128
  max_epochs: 1_000_000
  anneal_epochs: 10_000
  exploration: softmax
  T_max: 100.0
  T_min: 0.01
  # exploration: eps_greedy
  # eps_max: 1.0
  # eps_min: 0.01
  gamma: 0.95
  clip_gradients: true
  max_grad: 1.0
  num_burn_in: 100
  play_freq: 100
  num_play: 10
  validation_freq: 1000
  target_update_mode: hard
  soft_update_rate: 1.0e-5
  hard_update_freq: 1_000
  snapshot_freq: 1000
  optimizer:
    name: 'adam'
    lr: 1.0e-4
