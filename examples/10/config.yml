qnet:
  id: 'simple'
  depth: 5
  kernel_size: 3
  num_feat: 32
  bias: true
mode:
  id: 'zero'
  # blank = no normalization of rewards
  reward_norm:
  shape_rewards: false
replay:
  capacity: 1_000_000
  step_diff: 1
training:
  batch_size: 256
  max_epochs: 500_000
  eps_decay_time: 100_000
  eps_start: 1.0
  eps_end: 0.01
  gamma: 1.0
  clip_gradients: true
  max_grad: 1.0
  num_burn_in: 1_000
  play_freq: 100
  num_play: 10
  validation_freq: 100
  target_update_mode: hard
  soft_update_rate: 1.0e-4
  hard_update_freq: 20_000
  snapshot_freq: 1000
  optimizer:
    name: 'adam'
    lr: 1.0e-4
